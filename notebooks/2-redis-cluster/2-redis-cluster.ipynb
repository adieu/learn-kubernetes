{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 分布式Redis集群\n",
    "\n",
    "[Redis集群](https://github.com/kubernetes/kubernetes/tree/master/examples/redis) 是官方提供的案例，用来展示Kubernetes的特色。\n",
    "\n",
    "一个Redis集群当中包含一个Master节点，多个Slave节点，以及数个Sentinel节点。通过Sentinel节点实现了health checking以及failover，规避了单点故障。让我们试着在集群当中把它跑起来。\n",
    "\n",
    "## 部署Redis Master\n",
    "\n",
    "我们先来看看`redis-master`的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat redis-master.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在`redis-master`这个Pod里面，部署了两个容器，分别是一个Master，一个Sentinel。因为他们共享同一个网络环境，所以Sentinel可以用`$(hostname -i):6379`来找到Master，将它加入到监控列表当中，完成初始化。\n",
    "\n",
    "使用`kubectl`完成创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl create -f redis-master.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Sentinel服务\n",
    "\n",
    "Sentinel在Redis集群当中非常重要，当用户需要访问Redis集群时，先访问Sentinel，获取当前的Master和Slave地址，再进一步完成访问。\n",
    "\n",
    "我们将Sentinel暴露到集群当中去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl create -f redis-sentinel-service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Replication Controller\n",
    "\n",
    "上面创建的`redis-master`仅有一个Master，一个Sentinel，还远远称不上集群。让我们通过Replication Controller来增加更多的节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl create -f redis-controller.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl create -f redis-sentinel-controller.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要指出的是，`redis-master`已经满足上述Replication Controller中Selector的条件，而他们的Replica数均为1，所以它们并不会创建新的Pod，让我们来检查一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里体现了Replication Controller的工作原理，它仅仅是检查Selector得到的Pod数和目标数是否一致，如果不一致才会采取相应动作。\n",
    "\n",
    "让我们来创建更多的Replica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl scale rc redis --replicas=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl scale rc redis-sentinel --replicas=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些新创建的Redis服务器和Sentinel服务器，将通过刚才暴露给集群的`redis-sentinel`服务，找到现有的Sentinel服务器，并加入集群。\n",
    "\n",
    "## 删除手动创建的Pod\n",
    "\n",
    "最开始的`redis-master`是我们手动创建的，现在我们将它移除，而让集群自动管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl delete pods redis-master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，上面的两个Replication Controller会检查到Redis服务器和Sentinel服务器均只剩下两台，然后开始自动创建。而集群内部由于Master节点被移除，开始了选举流程，会有一个新的Redis服务器被选举为Master节点。\n",
    "\n",
    "让我们来验证一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl logs $SENTINEL_POD redis-sentinel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "通过上面简单几步，我们就在Kubernetes集群上搭建起了分布式的Redis集群。当有服务器故障时，Redis集群会利用Kubernetes的自动管理功能而恢复，服务并不会中断。当需要扩展Redis性能时，可以简单的下达scale指令增加Redis服务器数量，来承载更多的读操作。\n",
    "\n",
    "需要指出的是Redis集群并不提供Sharding的功能，所有的写操作都会发送到单一的Master节点上，所以它的主要功能是解决高可用的问题，虽然可以通过读写分离，Slave数量动态增加解决部分规模扩展的问题，但是并不全面。\n",
    "\n",
    "如果对具体的细节感兴趣，可以查看启动脚本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat image/run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 资源释放\n",
    "\n",
    "释放案例当中创建的资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl delete rc redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl delete rc redis-sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kubectl delete svc redis-sentinel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
